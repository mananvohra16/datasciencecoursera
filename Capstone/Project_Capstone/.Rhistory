geom_boxplot() + geom_point()
ggplot(data = mpg_vs_am, aes(x=am  , y=mpg))+
geom_boxplot() + geom_point()+
xlab("Transmission: 0 - Manual   1 - Automatic")+
ylab("Mpg")
data <- mtcars %>% mutate(
cyl = as.factor(cly),
vs = as.factor(vs),
am = as.factor(am),
gear = as.factor(am),
carb = as.factor(carb))
data <- mtcars %>% mutate(
cyl = as.factor(cyl),
vs = as.factor(vs),
am = as.factor(am),
gear = as.factor(am),
carb = as.factor(carb))
fit_all <- lm(mpg ~. , data = data)
summary(fit_all)$coef[,4]
fit_all <- lm(mpg ~. , data = data)
summary(fit_all)$coef[,4]
whcih.max(summary(fit_all)$coef[,4])
which.max(summary(fit_all)$coef[,4])
data <- data %>% select(-carb)
fit <- lm(mpg ~. , data = data)
summary(fit)$coef[,4]
data <- data %>% select(-carb)
data <- data %>% select(-carb)
fit <- lm(mpg ~. , data = data)
summary(fit)$coef[,4]
which.max(summary(fit)$coef[,4])
data <- data %>% select(-cyl); fit <- lm(mpg ~. , data = data); summary(fit)$coef[,4]
data <- data %>% select(-cyl); fit <- lm(mpg ~. , data = data); summary(fit)$coef[,4];which.max(summary(fit)$coef[,4])
which.max(summary(fit)$coef[,4])
data <- data %>% select(-vs); fit <- lm(mpg ~. , data = data); summary(fit)$coef[,4]; which.max(summary(fit)$coef[,4])
data <- data %>% select(-drat); fit <- lm(mpg ~. , data = data); summary(fit)$coef[,4]; which.max(summary(fit)$coef[,4])
data <- data %>% select(-disp); fit <- lm(mpg ~. , data = data); summary(fit)$coef[,4]; which.max(summary(fit)$coef[,4])
data <- data %>% select(-hp); fit <- lm(mpg ~. , data = data); summary(fit)$coef[,4]; which.max(summary(fit)$coef[,4])
summary(fit)
plot(fit)
par(mfrow = c(2, 2))
plot(fit)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
package.install(AppliedPredictiveModeling)
install.packages(AppliedPredictiveModeling)
install.packages(AppliedPredictiveModeling)
install.packages("AppliedPredictiveModeling")
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
install.packages("Caret")
install.packages("caret")
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
knitr::opts_chunk$set(echo = TRUE)
inTrain <- createDataPartition(y = train$classe, p = 0.7, list = F)
## Loading required libraries
library(caret)
library(randomForest)
#Downloading data
if (!file.exists('train.csv')) {
download.file(url = 'https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv',
destfile = 'train.csv', method = 'curl', quiet = TRUE)
}
if (!file.exists('test.csv')) {
download.file(url = 'https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv',
destfile = 'test.csv', method = 'curl', quiet = TRUE)
}
trainRaw <- read.csv('train.csv')
testRaw <- read.csv('test.csv')
#removing unrelated columns such as column number and time stamp
str(trainRaw)
#removing unrelated columns such as column number and time stamp
str(trainRaw)
train <- trainRaw[, 6:ncol(trainRaw)]
#Train and test data set creation
set.seed(23954)
inTrain <- createDataPartition(y = train$classe, p = 0.7, list = F)
install.packages("Rcpp")
knitr::opts_chunk$set(echo = TRUE)
inTrain <- createDataPartition(y = train$classe, p = 0.7, list = F)
## Loading required libraries
library(caret)
## Loading required libraries
library(caret)
library(randomForest)
#removing unrelated columns such as column number and time stamp
str(trainRaw)
#removing unrelated columns such as column number and time stamp
str(trainRaw)
train <- trainRaw[, 6:ncol(trainRaw)]
#Train and test data set creation
set.seed(23954)
inTrain <- createDataPartition(y = train$classe, p = 0.7, list = F)
training <- train[inTrain, ]
testing<- train[-inTrain, ]
#removing simillar variables
nzv <- nearZeroVar(train, saveMetrics = T)
#removing simillar variables
nzv <- nearZeroVar(train, saveMetrics = T)
keepFeat <- row.names(nzv[nzv$nzv == FALSE, ])
training <- training[, keepFeat]
#removing variables with all NAs
training <- training[, colSums(is.na(training)) == 0]
dim(training)
# 5 fold cross validation
modCtl <- trainControl(method = 'cv', number = 5)
#random forest modeling
set.seed(2384)
modRf <- train(classe ~. , data = training, method = 'rf', trControl = modCtl)
install.packages("e1071")
modRf <- train(classe ~. , data = training, method = 'rf', trControl = modCtl)
#random forest modeling
set.seed(2384)
modRf <- train(classe ~. , data = training, method = 'rf', trControl = modCtl, verbose = TRUE)
# 5 fold cross validation
modCtl <- trainControl(method = 'cv', number = 5,  verboseIter = TRUE)
#random forest modeling
set.seed(2384)
modRf <- train(classe ~. , data = training, method = 'rf', trControl = modCtl, verbose = TRUE)
# 5 fold cross validation
modCtl <- trainControl(method = 'cv', number = 1,  verboseIter = TRUE)
#random forest modeling
set.seed(2384)
modRf <- train(classe ~. , data = training, method = 'rf', trControl = modCtl, verbose = TRUE)
# 5 fold cross validation
modCtl <- trainControl(method = 'cv', number = 1,  verboseIter = TRUE)
#random forest modeling
set.seed(2384)
modRf <- train(classe ~. , data = training, method = 'rf', trControl = modCtl, verbose = TRUE)
# 5 fold cross validation
modCtl <- trainControl(method = 'cv', number = 2,  verboseIter = TRUE)
# 5 fold cross validation
modCtl <- trainControl(method = 'cv', number = 2,  verboseIter = TRUE)
#random forest modeling
set.seed(2384)
modRf <- train(classe ~. , data = training, method = 'rf', trControl = modCtl, verbose = TRUE)
modRf <- train(classe ~. , data = training, method = 'rf', verbose = TRUE)
# 5 fold cross validation
modCtl <- trainControl(method = 'cv', number = 1,  verboseIter = TRUE)
#random forest modeling
set.seed(2384)
modRf <- train(classe ~. , data = training, method = 'rf', trControl = modCtl, verbose = TRUE)
# 5 fold cross validation
modCtl <- trainControl(method = 'cv', number = 2,  verboseIter = TRUE)
#random forest modeling
set.seed(2384)
modRf <- train(classe ~. , data = training, method = 'rf', trControl = modCtl, verbose = TRUE)
# 5 fold cross validation
modCtl <- trainControl(method = 'cv', number = 2,  verboseIter = TRUE, allowParallel = TRUE)
#random forest modeling
set.seed(2384)
modRf <- train(classe ~. , data = training, method = 'rf', trControl = modCtl, verbose = TRUE)
modRf$finalModel
predRf <- predict(modRf, newdata = testing)
confusionMatrix(predRf, testing$classe)$table
predRf <- predict(modRf, newdata = testing)
confusionMatrix(predRf, testing$classe)$table
testing<- train[-inTrain, ]
confusionMatrix(predRf, testing$classe)$table
#removing simillar variables
nzv <- nearZeroVar(testing, saveMetrics = T)
keepFeat <- row.names(nzv[nzv$nzv == FALSE, ])
testing <- testing[, keepFeat]
#removing variables with all NAs
testing <- testing[, colSums(is.na(testing)) == 0]
dim(testing)
predRf <- predict(modRf, newdata = testing)
confusionMatrix(predRf, testing$classe)$table
confusionMatrix(predRf, testing$classe)$table[1]
# 5 fold cross validation
modCtl <- trainControl(method = 'cv', number = 5,  verboseIter = TRUE, allowParallel = TRUE)
#random forest modeling
set.seed(2384)
modRf <- train(classe ~. , data = training, method = 'rf', trControl = modCtl, verbose = TRUE)
modRf$finalModel
predRf <- predict(modRf, newdata = testing)
confusionMatrix(predRf, testing$classe)$table
confusionMatrix(predRf, testing$classe)$overall[1]
modGbm <- train(classe ~., data = training, method = 'gbm', trControl = modCtl, verbose = F)
confusionMatrix(predRf, testing$classe)$table
confusionMatrix(predRf, testing$classe)
)
predRf <- format(round(predict(modRf, newdata = testing)))
predRf <- predict(modRf, newdata = testing)
predRf
testing$classe
testing$classe
predRf
testing$classe
predRf <- as.character(predict(modRf, newdata = testing))
predRf <- as.character(predict(modRf, newdata = testing))
predRf
confusionMatrix(predRf, testing$classe)$table
predRf
testing$classe
class(testing$classe)
class(predRf)
predRf <- as.character(predict(modRf, newdata = testing))
confusionMatrix(predRf, testing$classe)$table
confusionMatrix(predRf, testing$classe)$overall[1]
table(predRf)
table(testing$classe)
table(factor(predRf, levels=min(test):max(test)),
factor(testing$classe, levels=min(test):max(test)))
table(factor(predRf, levels=min(testing$classe):max(testing$classe)),
factor(testing$classe, levels=min(testing$classe):max(testing$classe)))
predRf <- predict(modRf, newdata = testing)
class(predRf)
confusionMatrix(predRf, as.factor(testing$classe))$table
confusionMatrix(predRf, as.factor(testing$classe))$overall[1]
predRf <- predict(modRf, newdata = testing)
confusionMatrix(predRf, as.factor(testing$classe))$table
confusionMatrix(predRf, as.factor(testing$classe))$overall[1]
modGbm <- train(classe ~., data = training, method = 'gbm', trControl = modCtl, verbose = F)
modGbm$finalModel
predGbm <- predict(modGbm, newdata = testing)
confusionMatrix(predGbm, testing$classe)$table
predGbm <- predict(modGbm, newdata = testing)
confusionMatrix(predGbm, as.factor(testing$classe))$table
confusionMatrix(predGbm, as.factor(testing$classe))$overall[1]
predRfTest <- predict(modRf, newdata = testRaw)
predRfTest
predRfTest <- predict(modRf, newdata = testRaw)
predRfTest
predGbmTest <- predict(modGbm, newdata = testRaw)
table(predRfTest, predGbmTest)
knitr::opts_chunk$set(echo = TRUE)
#Test data error check
predRf <- predict(modRf, newdata = testing)
install.packages('leaflet')
knitr::opts_chunk$set(echo = TRUE)
library(leaflet)
library(htmltools)
m <- leaflet() %>%
addTiles() %>%  # Add default OpenStreetMap map tiles
addMarkers(lng=77.1025, lat=-28.7041, popup="The birthplace of R")
m  # Print the map
m <- leaflet() %>%
addTiles() %>%  # Add default OpenStreetMap map tiles
addMarkers(lng=77.1025, lat=-20.7041, popup="The birthplace of R")
m  # Print the map
m <- leaflet() %>%
addTiles() %>%  # Add default OpenStreetMap map tiles
addMarkers(lng=77.1025, lat=28.7041, popup="The birthplace of R")
m  # Print the map
dataset(iris)
datasets::iris
plotly(iris)
library(plotly)
plotly(iris)
plot_ly(iris)
plot_ly(data = iris)
plot_ly(economics, x = ~date, y = ~pop)
plot_ly(data = iris, x = ~Sepal.Length, y = ~Petal.Length)
plot_ly(economics, x = ~date, y = ~pop)
plot_ly(data = iris, x = ~Sepal.Length, y = ~Petal.Length)
plot_ly(data = economics, x = ~date, y = ~pop)
economics
iris
install.packages("webshot")
library(plotly)
plot_ly(data = iris, x = ~Sepal.Length, y = ~Petal.Length)
library(plotly)
trace_0 <- rnorm(100, mean = 5)
trace_1 <- rnorm(100, mean = 0)
trace_2 <- rnorm(100, mean = -5)
x <- c(1:100)
data <- data.frame(x, trace_0, trace_1, trace_2)
fig <- plot_ly(data, x = ~x, y = ~trace_0, name = 'trace 0', type = 'scatter', mode = 'lines')
fig <- fig %>% add_trace(y = ~trace_1, name = 'trace 1', mode = 'lines+markers')
fig <- fig %>% add_trace(y = ~trace_2, name = 'trace 2', mode = 'markers')
fig
shiny::runApp('project_9_3')
fithful
faithful
runApp('project_9_3')
iris
head(iiris)
head(iris)
runApp('project_9_3')
unique(iris$Species)
runApp('project_9_3')
runApp('project_9_3')
runApp('project_9_3')
runApp('project_9_3')
data <- iris[iris$Species == "setosa"]
data <- iris[Species == "setosa"]
data <- iris["Species" == "setosa"]
data
data <- iris[iris$Species == "setosa",]
data
runApp('project_9_3')
runApp('project_9_3')
runApp('project_9_3')
runApp('project_9_3')
runApp('project_9_3')
runApp('project_9_3')
iris$Species
iris$Species[]1
iris$Species[1}
iris$Species[1]
runApp('project_9_3')
runApp('project_9_3')
unique(iris$Species)
runApp('project_9_3')
runApp('project_9_3')
runApp('project_9_3')
runApp('project_9_3')
runApp()
runApp('project_9_3')
runApp('project_9_3')
getwd()
runApp('GitHub/datasciencecoursera/Course 9')
summary(iris)
library(ggplot)
ggplot(iris, aes(x=Sepal.Width, y=Sepal.Length)) +
geom_point() +
labs(title = "Sepal width vs Sepal length plot")
library(ggplot2)
ggplot(iris, aes(x=Sepal.Width, y=Sepal.Length)) +
geom_point() +
labs(title = "Sepal width vs Sepal length plot")
library(ggplot2)
fig <- ggplot(iris, aes(x=Sepal.Width, y=Sepal.Length)) +
geom_point() +
labs(title = "Sepal width vs Sepal length plot")
htmlwidgets::saveWidget(as.widget(fig), file = "demo.html")
knitr::opts_chunk$set(echo = TRUE)
stri_stats_general(Twitter)
library(stringi)
stri_stats_general(Blogs)
Loading data from the already downloaded files on the disk
```{r}
library(stringi)
Blogs <- suppressWarnings(readLines("./data/en_US/en_US.blogs.txt", skipNul = TRUE, encoding = "UTF-8"))
Twitter <- suppressWarnings(readLines("data/en_US/en_US.twitter.txt", skipNul = TRUE, encoding = "UTF-8"))
News <- suppressWarnings(readLines("data/en_US/en_US.news.txt", skipNul = TRUE, encoding = "UTF-8"))
```
library(stringi)
Blogs <- suppressWarnings(readLines("./data/en_US/en_US.blogs.txt", skipNul = TRUE, encoding = "UTF-8"))
Twitter <- suppressWarnings(readLines("data/en_US/en_US.twitter.txt", skipNul = TRUE, encoding = "UTF-8"))
News <- suppressWarnings(readLines("data/en_US/en_US.news.txt", skipNul = TRUE, encoding = "UTF-8"))
head(Blogs,5)
stri_stats_general(Blogs)
dtm1 <- TermDocumentMatrix(data_clean)
knitr::opts_chunk$set(echo = TRUE)
library(plyr)
library(magrittr)
library(stringr)
library(stringi)
library(tm)
library(RWeka)
knitr::opts_chunk$set(echo = TRUE)
library(plyr)
library(magrittr)
library(stringr)
library(stringi)
library(tm)
library(RWeka)
library(SnowballC)
library(ggplot2)
dtm1 <- TermDocumentMatrix(data_clean)
set.seed(9999)
Sub_Twitter <- sample(Twitter, size = 5000, replace = TRUE)
Sub_Blogs <- sample(Blogs, size = 5000, replace = TRUE)
Sub_News <- sample(News, size = 5000, replace = TRUE)
#Union the sample datasets
Dataset <- c(Sub_Twitter, Sub_Blogs, Sub_News)
Corpus <- Corpus(VectorSource(Dataset))
data_clean <- removePunctuation(Dataset)
data_clean <- tolower(data_clean)
data_clean <- removeNumbers(data_clean)
data_clean <- stripWhitespace(data_clean)
data_clean <- stemDocument(data_clean)
saveRDS(data_clean, file = "data_clean.RData")
dtm1 <- TermDocumentMatrix(data_clean)
corpus <- tm_map(Dataset, toSpace, "/|@|//|$|:|:)|*|&|!|?|_|-|#|")
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
corpus <- tm_map(Dataset, toSpace, "/|@|//|$|:|:)|*|&|!|?|_|-|#|")
set.seed(9999)
Sub_Twitter <- sample(Twitter, size = 5000, replace = TRUE)
Sub_Blogs <- sample(Blogs, size = 5000, replace = TRUE)
Sub_News <- sample(News, size = 5000, replace = TRUE)
#Union the sample datasets
dataset <- c(Sub_Twitter, Sub_Blogs, Sub_News)
corpus <- VCorpus(VectorSource(dataset))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
corpus <- tm_map(corpus, toSpace, "/|@|//|$|:|:)|*|&|!|?|_|-|#|")
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, stopwords())
corpus <- tm_map(corpus, stemDocument)
corpus <- tm_map(corpus, stripWhitespace)
saveRDS(data_clean, file = "data_clean.RData")
dtm1 <- TermDocumentMatrix(data_clean)
dtm1 <- TermDocumentMatrix(data_clean)
dtm1 <- TermDocumentMatrix(corpus)
bigram <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
dtm2 <- TermDocumentMatrix(corpus, control = list(tokenize = bigram))
trigram <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
dtm3 <- TermDocumentMatrix(corpus, control = list(tokenize = trigram))
ggplot(dfFreq1[1:20, ], aes(word, freq)) +
geom_bar(stat="identity", fill="red", colour="red") +
theme(axis.text.x=element_text(angle=45, hjust=1)) + ggtitle("1-Gram Frequency")
freq1 <- rowSums(as.matrix(dtm1))
freq1 <- sort(freq1, decreasing = TRUE)
dfFreq1 <- data.frame(word = names(freq1), freq=freq1)
ggplot(dfFreq1[1:20, ], aes(word, freq)) +
geom_bar(stat="identity", fill="red", colour="red") +
theme(axis.text.x=element_text(angle=45, hjust=1)) + ggtitle("1-Gram Frequency")
freq1 <- rowSums(as.matrix(dtm1))
freq1 <- sort(freq1, decreasing = TRUE)
dfFreq1 <- data.frame(word = names(freq1), freq=freq1)
ggplot(dfFreq1[1:20, ], aes(word, freq)) +
geom_bar(stat="identity", fill="red", colour="green") +
theme(axis.text.x=element_text(angle=45, hjust=1)) + ggtitle("1-Gram Frequency")
freq1 <- rowSums(as.matrix(dtm1))
freq1 <- sort(freq1, decreasing = TRUE)
dfFreq1 <- data.frame(word = names(freq1), freq=freq1)
ggplot(dfFreq1[1:20, ], aes(word, freq)) +
geom_bar(stat="identity", fill="green", colour="green") +
theme(axis.text.x=element_text(angle=45, hjust=1)) + ggtitle("1-Gram Frequency")
freq1 <- rowSums(as.matrix(dtm1))
freq1 <- sort(freq1, decreasing = TRUE)
dfFreq1 <- data.frame(word = names(freq1), freq=freq1)
ggplot(dfFreq1[1:20, ], aes(word, freq)) +
geom_bar(stat="identity", fill="orange", colour="green") +
theme(axis.text.x=element_text(angle=45, hjust=1)) + ggtitle("1-Gram Frequency")
ggplot(dfFreq1[1:20, ], aes(word, freq)) +
geom_bar(stat="identity", fill="orange", colour="orange") +
theme(axis.text.x=element_text(angle=45, hjust=1)) + ggtitle("1-Gram Frequency")
freq2 <- rowSums(as.matrix(dtm2))
freq2 <- rowSums(as.matrix(dtm2))
dtm1 <- TermDocumentMatrix(corpus)
bigram <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
dtm2 <- TermDocumentMatrix(corpus, control = list(tokenize = bigram))
trigram <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
dtm3 <- TermDocumentMatrix(corpus, control = list(tokenize = trigram))
freq2 <- rowSums(as.matrix(dtm2))
freq2 <- rowSums(as.matrix(dtm2))
gc()
freq2 <- rowSums(as.matrix(dtm2))
freq3 <- rowSums(as.matrix(dtm3))
freq3 <- rowSums(as.matrix(dtm3))
library(wordcloud)
install.packages("wordcloud")
library(wordcloud)
text_clean <- readRDS("./text_clean.RData")
suppressWarnings(wordcloud(Sub_Blogs, max.words = 60, random.order = FALSE,rot.per=0.35, use.r.layout=FALSE,colors=brewer.pal(8, "Dark2")))
suppressWarnings(wordcloud(Sub_Blogs, max.words = 40, random.order = FALSE,rot.per=0.35, use.r.layout=FALSE,colors=brewer.pal(8, "Dark2")))
suppressWarnings(wordcloud(Sub_Blogs, max.words = 50, random.order = FALSE,rot.per=0.35, use.r.layout=FALSE,colors=brewer.pal(8, "Dark2")))
```{r}
suppressWarnings(wordcloud(Sub_News, max.words = 50, random.order = FALSE,rot.per=0.35, use.r.layout=FALSE,colors=brewer.pal(8, "Dark2")))
suppressWarnings(wordcloud(Sub_Twitter, max.words = 50, random.order = FALSE,rot.per=0.35, use.r.layout=FALSE,colors=brewer.pal(8, "Dark2")))
wordcloud(Sub_Twitter, max.words = 50, random.order = FALSE,rot.per=0.35, use.r.layout=FALSE,colors=brewer.pal(8, "Dark2"))
wordcloud(Sub_News, max.words = 50, random.order = FALSE,rot.per=0.35, use.r.layout=FALSE,colors=brewer.pal(8, "Dark2"))
wordcloud(text_clean, max.words = 60, random.order = FALSE,rot.per=0.35, use.r.layout=FALSE,colors=brewer.pal(8, "Dark2"))
wordcloud(corpus, max.words = 60, random.order = FALSE,rot.per=0.35, use.r.layout=FALSE,colors=brewer.pal(8, "Dark2"))
wordcloud(corpus, max.words = 50, random.order = FALSE,rot.per=0.35, use.r.layout=FALSE,colors=brewer.pal(8, "Dark2"))
wordcloud(corpus, max.words = 50, random.order = FALSE,rot.per=0.35, use.r.layout=FALSE,random.color = TRUE)
wordcloud(corpus, max.words = 50, random.order = FALSE,rot.per=0.35, use.r.layout=FALSE,random.color)
wordcloud(corpus, max.words = 50, random.order = FALSE,rot.per=0.35, use.r.layout=FALSE,random.color = ("Green", "Yellow", "Blue", "Red", "Orange"))
wordcloud(corpus, max.words = 50, random.order = FALSE,rot.per=0.35, use.r.layout=FALSE,random.color = (["Green", "Yellow", "Blue", "Red", "Orange"])
wordcloud(corpus, max.words = 50, random.order = FALSE,rot.per=0.35, use.r.layout=FALSE,random.color = ["Green", "Yellow", "Blue", "Red", "Orange"])
wordcloud(corpus, max.words = 50, random.order = FALSE,rot.per=0.35, use.r.layout=FALSE,random.color = c("Green", "Yellow", "Blue", "Red", "Orange"))
wordcloud(corpus, max.words = 50, random.order = FALSE,rot.per=0.35, use.r.layout=FALSE,random.color = False)
wordcloud(corpus, max.words = 50, random.order = FALSE,rot.per=0.35, use.r.layout=FALSE,random.color = FALSE)
wordcloud(corpus, max.words = 50, random.order = FALSE,rot.per=0.35, use.r.layout=FALSE)
wordcloud(corpus, max.words = 50, random.order = FALSE,rot.per=0.35, use.r.layout=FALSE,colors=brewer.pal(8, "Pastel2"))
wordcloud(corpus, max.words = 50, random.order = FALSE,rot.per=0.35, use.r.layout=FALSE,colors=brewer.pal(8, "Accent"))
wordcloud(corpus, max.words = 50, random.order = FALSE,rot.per=0.35, use.r.layout=FALSE,colors=brewer.pal(8, "Paired"))
wordcloud(corpus, max.words = 50, random.order = FALSE,rot.per=0.35, use.r.layout=FALSE,colors=brewer.pal(12, "Paired"))
knitr::opts_chunk$set(echo = TRUE)
library(stringi)
library(tm)
library(ggplot2)
library(wordcloud)
options(mc.cores=1)
corpus_matrix <- TermDocumentMatrix(corpus)
getWordFrequency <- function(tdm) {
freq <- sort(rowSums(as.matrix(tdm)), decreasing = TRUE)
return(data.frame(word = names(freq), freq = freq))
}
freq1 <- getWordFrequency(removeSparseTerms(corpus_matrix, 0.99))
ggplot(freq1[1:20,], aes(reorder(word, -freq), freq)) +
labs(x = "Top 20 Words", y = "Frequency") +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
geom_bar(stat = "identity", fill = "blue")
shiny::runApp('Project_Capstone')
runApp('Project_Capstone')
runApp('Project_Capstone')
getwd()
setwd("~/GitHub/datasciencecoursera/Capstone")
shiny::runApp('Project_Capstone')
runApp('Project_Capstone')
runApp('Project_Capstone')
install.packages("vctrs")
shiny::runApp()
install.packages("ellipsis")
install.packages("ellipsis")
install.packages("ellipsis")
install.packages("ellipsis")
install.packages("ellipsis")
install.packages("ellipsis")
shiny::runApp()
install.packages("ellipsis")
remove.packages("tidyverse", lib="~/R/win-library/4.0")
