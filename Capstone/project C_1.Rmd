---
title: "Project Capstone 1"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(stringi)
library(tm)
library(ggplot2)
library(wordcloud)
```

#Introduction

The goal of this project is just to display that you've gotten used to working with the data and that you are on track to create your prediction algorithm. Please submit a report on R Pubs (http://rpubs.com/) that explains your exploratory analysis and your goals for the eventual app and algorithm. This document should be concise and explain only the major features of the data you have identified and briefly summarize your plans for creating the prediction algorithm and Shiny app in a way that would be understandable to a non-data scientist manager. You should make use of tables and plots to illustrate important summaries of the data set. The motivation for this project is to: 

1. Demonstrate that you've downloaded the data and have successfully loaded it in.
2. Create a basic report of summary statistics about the data sets.
3. Report any interesting findings that you amassed so far.
4. Get feedback on your plans for creating a prediction algorithm and Shiny app.

# Loading Data

Loading data from the already downloaded files on the disk

```{r}
Blogs <- suppressWarnings(readLines("./data/en_US/en_US.blogs.txt", skipNul = TRUE, encoding = "UTF-8"))
Twitter <- suppressWarnings(readLines("data/en_US/en_US.twitter.txt", skipNul = TRUE, encoding = "UTF-8"))
News <- suppressWarnings(readLines("data/en_US/en_US.news.txt", skipNul = TRUE, encoding = "UTF-8"))
```

# Data summary

## Blog
 Displaying first 5 elements and number of summary of data set including number of lines, number of character etc.
```{r}
head(Blogs,5)
stri_stats_general(Blogs)
```

## Twitter
Displaying first 5 elements and number of summary of data set including number of lines, number of character etc.

```{r}
head(Twitter,5)
stri_stats_general(Twitter)
```

## News
Displaying first 5 elements and number of summary of data set including number of lines, number of character etc.

```{r}
head(News,5)
stri_stats_general(News)
```

As the data size is large we are going to create sample data set from it to work with


```{r}
set.seed(9999)
Sub_Twitter <- sample(Twitter, size = 5000, replace = TRUE)
Sub_Blogs <- sample(Blogs, size = 5000, replace = TRUE)
Sub_News <- sample(News, size = 5000, replace = TRUE)

#Union the sample datasets
dataset <- c(Sub_Twitter, Sub_Blogs, Sub_News)
corpus <- VCorpus(VectorSource(dataset))
```

# Cleaning Data set

- Removing punctuation 
- Converting to lower
- Removing numbers
- Removing white space
- stem common words
 
```{r}
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
corpus <- tm_map(corpus, toSpace, "/|@|//|$|:|:)|*|&|!|?|_|-|#|")
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, stopwords())
corpus <- tm_map(corpus, stemDocument)
corpus <- tm_map(corpus, stripWhitespace)
saveRDS(corpus, file = "data_clean.RData")

```


# Data Exploration and Visualization

Now we will be creating word cloud for all the 3 datasets and the corpus to look at the top 50 words by frequency

```{r}
text_clean <- readRDS("./data_clean.RData")
```

Blog word cloud 

```{r}
wordcloud(Sub_Blogs, max.words = 50, random.order = FALSE,rot.per=0.35, use.r.layout=FALSE,colors=brewer.pal(8, "Dark2"))
```

News word cloud

```{r}
wordcloud(Sub_News, max.words = 50, random.order = FALSE,rot.per=0.35, use.r.layout=FALSE,colors=brewer.pal(8, "Dark2"))
```

Twitter word cloud

```{r}
wordcloud(Sub_Twitter, max.words = 50, random.order = FALSE,rot.per=0.35, use.r.layout=FALSE,colors=brewer.pal(8, "Dark2"))
```

Full dataset word cloud

```{r}
wordcloud(corpus, max.words = 50, random.order = FALSE,rot.per=0.35, use.r.layout=FALSE,colors=brewer.pal(12, "Paired"))

```

Corpus histogram 

```{r}

options(mc.cores=1)

corpus_matrix <- TermDocumentMatrix(corpus)

getWordFrequency <- function(tdm) {
  freq <- sort(rowSums(as.matrix(tdm)), decreasing = TRUE)
  return(data.frame(word = names(freq), freq = freq))
}

freq1 <- getWordFrequency(removeSparseTerms(corpus_matrix, 0.99))

ggplot(freq1[1:20,], aes(reorder(word, -freq), freq)) +
    labs(x = "Top 20 Words", y = "Frequency") +
    theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
    geom_bar(stat = "identity", fill = "blue")
```

# Conclusion and Next steps

- From the above word cloud diagrams we can conclude that all 3 datasets have different frequencies for different words.
- Rather than working on whole data date we will be working with the sample dataset to find the second word relation between frequencies with bigrams and trigrams and then apply that to whole dataset.
- we will be building a shinly applicatoin for this.




